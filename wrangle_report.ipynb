{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fd34ae",
   "metadata": {},
   "source": [
    "# Wrangle Report For WeRateDogs Project\n",
    "\n",
    "## By: Emmanuel Eshun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7cf28",
   "metadata": {},
   "source": [
    "This project has been a bit challenging and time consuming for me. But in all it helped me learned new things each time I tackle a section. \n",
    "\n",
    "Starting, the had only the twitter_archive dataset readily available for download which was an unclean dataset that needs a lot of work. I later on downloaded the  image_predictions dataset programatically in a ‘tsv’ format. I used the get function to access and write the contents to an empty file. Then jupyter’s panda package to load the two dataset in the working environment.\n",
    "Moving on to the next file, I was required to use the twitter API call but had some issues with the developer account so i proceeded with the data provided.The format of this was a ‘txt’ format thus “tweet-json.txt”. I had to create an empty list and loop through thee file to read lines and store the ‘tweet_id’, ‘retweet_count’ and ‘favorite_count’. I later used their corresponding tweet id’s to create a pandas dataframe..\n",
    "\n",
    "In total I had 3 different but similar dataset to work with. Thus, assess for quality issues and clean. Most of the issues came from the twitter archive dataset which had a lot of data in there comparing to that of the remaing two datasets. Because of its lengthy nature it required a lot of work and much of the time was spent on it. Some of these issues was identified visually whereas the other was also programmatically with the help of pandas’ functions like the head and sample. Others also required a little more analysis. Value_counts and info functions were dominantly used. Tidiness issues was also addressed by joining tables and combining some features into a single table.\n",
    "\n",
    "Final stage was the more code based which was cleaning. For each issure, the define, code, test format was implemented where I first define the issue, I solve the issue with code and finally test to see if it worked. A lot of operations was done on these dataset including removing null rows and also removing columns from various data sets.\n",
    "\n",
    "Overall, this project taught me how to deal with all the complications that arise when building your own datasets, identify various quality and organization-related concerns, and then fix those problems so that the datasets are analysis-worthy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
